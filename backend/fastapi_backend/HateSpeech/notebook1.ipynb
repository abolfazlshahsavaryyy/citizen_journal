{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "d5328041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "2fd9ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def read_data(path):\n",
    "#     df = pd.read_csv(path)\n",
    "#     df = df.sample(frac=0.1, random_state=42)  # random_state for reproducibility\n",
    "#     df = df.reset_index(drop=True)  # Optional: reset index after sampling\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "bcc0df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path='/home/abolfazl/Documents/CitizenJournal/citizen_journal/backend/fastapi_backend/HateSpeech/HateSpeechDatasetBalanced.csv'\n",
    "# df=read_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539471ea",
   "metadata": {},
   "source": [
    "### BertTokenizer: \n",
    " Splits and converts your input text (like a sentence) into tokens that BERT can understand.\n",
    "\n",
    "### BertModel:\n",
    " The actual pre-trained BERT model that turns the tokens into useful numeric vectors (called embeddings).\n",
    "\n",
    "### bert-base-uncased:\n",
    " A version of BERT that has been trained on English text, where all words are lowercase.\n",
    "\n",
    "### model.eval():\n",
    " Tells the model to run in inference mode (not training mode). This saves memory and speeds up processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "3768292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model.eval()  # inference mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "5f5ca9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cls_embedding(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "#     return cls_embedding.squeeze().numpy()  # shape: (768,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "810c0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = []\n",
    "\n",
    "# for text in tqdm(df['Content'], desc=\"Extracting BERT embeddings\"):\n",
    "#     try:\n",
    "#         vector = get_cls_embedding(text)\n",
    "#         embeddings.append(vector)\n",
    "#     except Exception as e:\n",
    "#         embeddings.append(None)  # in case of any failure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "402c65fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the list of embeddings into a DataFrame\n",
    "# embedding_df = pd.DataFrame(embeddings)\n",
    "\n",
    "# # Rename the columns before merging\n",
    "# embedding_df.columns = [f'embedding_{i}' for i in range(embedding_df.shape[1])]\n",
    "\n",
    "# # Merge with the original DataFrame\n",
    "# df = pd.concat([df, embedding_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "9ddae5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = '/home/abolfazl/Documents/CitizenJournal/citizen_journal/backend/fastapi_backend/HateSpeech/embedded_data.csv'\n",
    "# df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "5a745b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_758</th>\n",
       "      <th>embedding_759</th>\n",
       "      <th>embedding_760</th>\n",
       "      <th>embedding_761</th>\n",
       "      <th>embedding_762</th>\n",
       "      <th>embedding_763</th>\n",
       "      <th>embedding_764</th>\n",
       "      <th>embedding_765</th>\n",
       "      <th>embedding_766</th>\n",
       "      <th>embedding_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you should be deeply embarrassed... by not ful...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.278104</td>\n",
       "      <td>0.333025</td>\n",
       "      <td>-0.294160</td>\n",
       "      <td>-0.145966</td>\n",
       "      <td>-0.485665</td>\n",
       "      <td>-0.786977</td>\n",
       "      <td>0.459131</td>\n",
       "      <td>0.544359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046933</td>\n",
       "      <td>-0.309165</td>\n",
       "      <td>-0.127641</td>\n",
       "      <td>-0.448980</td>\n",
       "      <td>-0.140215</td>\n",
       "      <td>0.193676</td>\n",
       "      <td>-0.034632</td>\n",
       "      <td>-0.019996</td>\n",
       "      <td>0.560498</td>\n",
       "      <td>0.340829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do not make me make you fall in love with a bi...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.133017</td>\n",
       "      <td>0.202193</td>\n",
       "      <td>-0.421173</td>\n",
       "      <td>-0.009662</td>\n",
       "      <td>-0.201444</td>\n",
       "      <td>-0.419096</td>\n",
       "      <td>0.644618</td>\n",
       "      <td>0.595702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270720</td>\n",
       "      <td>-0.357238</td>\n",
       "      <td>0.121133</td>\n",
       "      <td>-0.065833</td>\n",
       "      <td>0.152818</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>0.077813</td>\n",
       "      <td>-0.305431</td>\n",
       "      <td>-0.050150</td>\n",
       "      <td>0.260495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trump america is anti immigrant sexual activit...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.138762</td>\n",
       "      <td>0.005806</td>\n",
       "      <td>-0.092694</td>\n",
       "      <td>-0.007501</td>\n",
       "      <td>-0.443966</td>\n",
       "      <td>-0.109705</td>\n",
       "      <td>0.614334</td>\n",
       "      <td>0.544135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178341</td>\n",
       "      <td>-0.271666</td>\n",
       "      <td>0.061014</td>\n",
       "      <td>0.067001</td>\n",
       "      <td>0.656415</td>\n",
       "      <td>0.115183</td>\n",
       "      <td>-0.189214</td>\n",
       "      <td>-0.475422</td>\n",
       "      <td>0.309695</td>\n",
       "      <td>0.382447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you guys are clearly a pole smoker please get ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.406987</td>\n",
       "      <td>0.299873</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>-0.191551</td>\n",
       "      <td>-0.434151</td>\n",
       "      <td>-0.539173</td>\n",
       "      <td>0.511990</td>\n",
       "      <td>0.659850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038749</td>\n",
       "      <td>-0.406413</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>-0.162931</td>\n",
       "      <td>0.342772</td>\n",
       "      <td>-0.140089</td>\n",
       "      <td>-0.001462</td>\n",
       "      <td>-0.489326</td>\n",
       "      <td>0.283278</td>\n",
       "      <td>0.042651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oh come along the only reason people like stri...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.209989</td>\n",
       "      <td>0.294031</td>\n",
       "      <td>0.037456</td>\n",
       "      <td>-0.131755</td>\n",
       "      <td>-0.274220</td>\n",
       "      <td>-0.452337</td>\n",
       "      <td>0.039897</td>\n",
       "      <td>0.521467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015279</td>\n",
       "      <td>-0.253148</td>\n",
       "      <td>-0.329311</td>\n",
       "      <td>-0.145785</td>\n",
       "      <td>0.218858</td>\n",
       "      <td>0.116727</td>\n",
       "      <td>-0.372996</td>\n",
       "      <td>-0.278894</td>\n",
       "      <td>0.474181</td>\n",
       "      <td>0.245281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Content  Label  embedding_0  \\\n",
       "0  you should be deeply embarrassed... by not ful...      1     0.278104   \n",
       "1  do not make me make you fall in love with a bi...      0    -0.133017   \n",
       "2  trump america is anti immigrant sexual activit...      1    -0.138762   \n",
       "3  you guys are clearly a pole smoker please get ...      1     0.406987   \n",
       "4  oh come along the only reason people like stri...      0     0.209989   \n",
       "\n",
       "   embedding_1  embedding_2  embedding_3  embedding_4  embedding_5  \\\n",
       "0     0.333025    -0.294160    -0.145966    -0.485665    -0.786977   \n",
       "1     0.202193    -0.421173    -0.009662    -0.201444    -0.419096   \n",
       "2     0.005806    -0.092694    -0.007501    -0.443966    -0.109705   \n",
       "3     0.299873     0.009246    -0.191551    -0.434151    -0.539173   \n",
       "4     0.294031     0.037456    -0.131755    -0.274220    -0.452337   \n",
       "\n",
       "   embedding_6  embedding_7  ...  embedding_758  embedding_759  embedding_760  \\\n",
       "0     0.459131     0.544359  ...       0.046933      -0.309165      -0.127641   \n",
       "1     0.644618     0.595702  ...       0.270720      -0.357238       0.121133   \n",
       "2     0.614334     0.544135  ...      -0.178341      -0.271666       0.061014   \n",
       "3     0.511990     0.659850  ...      -0.038749      -0.406413       0.051500   \n",
       "4     0.039897     0.521467  ...      -0.015279      -0.253148      -0.329311   \n",
       "\n",
       "   embedding_761  embedding_762  embedding_763  embedding_764  embedding_765  \\\n",
       "0      -0.448980      -0.140215       0.193676      -0.034632      -0.019996   \n",
       "1      -0.065833       0.152818       0.050402       0.077813      -0.305431   \n",
       "2       0.067001       0.656415       0.115183      -0.189214      -0.475422   \n",
       "3      -0.162931       0.342772      -0.140089      -0.001462      -0.489326   \n",
       "4      -0.145785       0.218858       0.116727      -0.372996      -0.278894   \n",
       "\n",
       "   embedding_766  embedding_767  \n",
       "0       0.560498       0.340829  \n",
       "1      -0.050150       0.260495  \n",
       "2       0.309695       0.382447  \n",
       "3       0.283278       0.042651  \n",
       "4       0.474181       0.245281  \n",
       "\n",
       "[5 rows x 770 columns]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the saved dataset\n",
    "df = pd.read_csv('/home/abolfazl/Documents/CitizenJournal/citizen_journal/backend/fastapi_backend/HateSpeech/embedded_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "05ce7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: columns embedding_0 to embedding_767\n",
    "X = df[[f'embedding_{i}' for i in range(768)]].values\n",
    "\n",
    "# Labels: assume your label column is 'label'\n",
    "y = df['Label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "bc2b9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "638f8bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7291 - loss: 0.5293 - val_accuracy: 0.7763 - val_loss: 0.4773\n",
      "Epoch 2/50\n",
      "\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7668 - loss: 0.4758 - val_accuracy: 0.7879 - val_loss: 0.4564\n",
      "Epoch 3/50\n",
      "\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7808 - loss: 0.4548 - val_accuracy: 0.7871 - val_loss: 0.4423\n",
      "Epoch 4/50\n",
      "\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7871 - loss: 0.4452 - val_accuracy: 0.7959 - val_loss: 0.4289\n",
      "Epoch 5/50\n",
      "\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7973 - loss: 0.4309 - val_accuracy: 0.7736 - val_loss: 0.4570\n",
      "Epoch 6/50\n",
      "\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8022 - loss: 0.4238 - val_accuracy: 0.8014 - val_loss: 0.4266\n",
      "Epoch 7/50\n",
      "\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8056 - loss: 0.4151 - val_accuracy: 0.8017 - val_loss: 0.4176\n",
      "Epoch 8/50\n",
      "\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8107 - loss: 0.4041 - val_accuracy: 0.7972 - val_loss: 0.4411\n",
      "Epoch 9/50\n",
      "\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8131 - loss: 0.4014 - val_accuracy: 0.8041 - val_loss: 0.4196\n",
      "Epoch 10/50\n",
      "\u001b[1m473/473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8210 - loss: 0.3875 - val_accuracy: 0.7967 - val_loss: 0.4425\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(768,)),\n",
    "    \n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "6455a282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - accuracy: 0.8117 - loss: 0.4132\n",
      "Test Accuracy: 0.8117\n"
     ]
    }
   ],
   "source": [
    "# Accuracy on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "f6716813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.77      0.80      2347\n",
      "           1       0.79      0.85      0.82      2375\n",
      "\n",
      "    accuracy                           0.81      4722\n",
      "   macro avg       0.81      0.81      0.81      4722\n",
      "weighted avg       0.81      0.81      0.81      4722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "7ade78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('/home/abolfazl/Documents/CitizenJournal/citizen_journal/backend/fastapi_backend/HateSpeech/hate_speech_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ffda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
